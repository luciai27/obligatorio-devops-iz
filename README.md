# ğŸ—³ï¸ Obligatorio Voting App

## ğŸ› ï¸ Herramientas
- **Repositorio:** GitHub  
- **CI/CD:** GitHub Actions  
- **AnÃ¡lisis de cÃ³digo estÃ¡tico:** SonarQube  
- **Cloud:** AWS  
- **Infraestructura como CÃ³digo (IaC):** Terraform  
- **Testing:** JMeter
- **Serverless:** Lambda 

---
## ğŸ” Prerequisitos
Estas variables deben estar configuradas como *Secrets* en GitHub:

- `AWS_ACCESS_KEY_ID`
- `AWS_REGION`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_SESSION_TOKEN`
- `BUCKET_NAME` (nombre Ãºnico del bucket S3 en donde se guardarÃ¡n los tfstates.)
- `EMAIL_USER`
- `EMAIL_PASS`
- `REPO_OWNER_MAIL`
- `SONAR_TOKEN`

Los primeros cuatro secretos corresponden a configuraciones de AWS, por lo que siempre son necesarios para poder acceder correctamente a los servicios.

El Bucket Name es necesario ya que, al ser dos personas las que estamos trabajando en el proyecto y dado que los buckets de S3 deben tener nombres Ãºnicos, no es posible utilizar un mismo nombre en cuentas diferentes. Esto fue algo que se tuvo que parametrizar (y, por lo tanto, el mismo debe ser creado manualmente **antes** de correr el pipeline).

Los secretos de Email User, Email Pass y Repo Owner Mail son necesarios para el envÃ­o de correo cuando se crea un Pull Request y cuando finaliza lambda.

El Sonar Token es necesario para la realizaciÃ³n del anÃ¡lisis de cÃ³digo de SonarQube.


---

## ğŸŒ¿ Estrategia Git Flow

La estrategia elegida fue **Git Flow**. Si bien entendemos que la estrategia **Trunk Based** tiene caracterÃ­sticas Ãºtiles (promueve integraciÃ³n continua, especialmente Ãºtil en proyectos pequeÃ±os), decidimos utilizar **Git Flow** ya que nos permite observar mÃ¡s atentamente los cambios realizados a la rama principal.

Dado que todavÃ­a estamos aprendiendo cÃ³mo utilizar las tecnologÃ­as enseÃ±adas en clase, consideramos que un monitoreo mÃ¡s a fondo de lo que se incorpora a la rama principal es la estrategia que mÃ¡s se alinea con nuestra forma de trabajo. Al utilizar esta estrategia, sabemos que lo que se integra a `main` estÃ¡ funcionando correctamente.

### âœ… Entornos bien definidos y separados
El proyecto tiene ramas bien diferenciadas que se alinean con Git Flow:
- `dev`: para desarrollo
- `test`: para validaciÃ³n antes de producciÃ³n
- `main`: versiÃ³n estable y en producciÃ³n

### ğŸ“¦ Control sobre versiones y despliegues
Git Flow permite:
- Controlar cuÃ¡ndo se libera una nueva versiÃ³n
- Aplicar hotfixes sin afectar `dev`
- Mantener la estabilidad en `main` mientras se desarrollan nuevas funcionalidades

### ğŸ” IntegraciÃ³n con flujos CI/CD por ramas
- La app genera imÃ¡genes por rama (`dev`, `test`, `main`)
- Cada rama despliega en su entorno especÃ­fico
- Git Flow encaja naturalmente con pipelines CI/CD basados en tags por rama

### ğŸ›¡ï¸ AislaciÃ³n de features y bugs
- Ramas especÃ­ficas para nuevas features sin romper `dev`
- Hotfixes crÃ­ticos directamente sobre `main`
- Mayor seguridad antes de llegar a producciÃ³n

---



## ğŸ“ Estrategia de Repositorio para Infraestructura

Decidimos usar **el mismo repositorio** para la carpeta de infraestructura. 

Esto nos resulta mÃ¡s prÃ¡ctico para un proyecto pequeÃ±o como este, ya que podemos realizar cambios tanto en la aplicaciÃ³n como en la infraestructura desde un mismo lugar. Si el proyecto fuera mÃ¡s grande, sÃ­ considerarÃ­amos separar el cÃ³digo de infraestructura en un repositorio exclusivo para facilitar su reutilizaciÃ³n.

---

## ğŸ§± Arquitectura

![Architecture diagram](architecture.excalidraw.png)

Componentes:
- ğŸ Front-end en [Python](/vote): permite votar entre dos opciones
- ğŸ§  [Redis](https://hub.docker.com/_/redis/): almacena los votos temporales
- âš™ï¸ [Worker en .NET](/worker): consume votos desde Redis y los guarda en...
- ğŸ›¢ï¸ [Postgres](https://hub.docker.com/_/postgres/): base de datos persistente
- ğŸ“Š Web app [Node.js](/result): muestra resultados de la votaciÃ³n en tiempo real

---

## ğŸš€ Flow de CI/CD

1. **Push a una rama (`dev`, `test`, `main`)**
   - Se configuran credenciales AWS
   - Se crean repositorios ECR para imÃ¡genes
   - Se genera una nueva imagen Docker con tag Ãºnico
   - Se sube la imagen a ECR con tag de entorno
   - Se actualiza el archivo `docker-compose.generated.yml` con el tag generado
   - El archivo `docker-compose.generated.yml` se sube a bucket S3
   - Se crea repositorio para Lambda
   - Se genera una imagen para backup utilizando Lambda
   - Se sube la imagen a ECR
   - Se crea la infraestructura comÃºn a todos los ambientes (network)
   - Se crea la infrastructura correspondiente al ambiente del push
   - Se despliegan manifiestos K8s
   - Se realiza testing de carga en ALBs creados por K8s (Vote y Result)
   - Se invoca funciÃ³n Lambda

   - Se envÃ­a notificaciÃ³n por correo electrÃ³nico

ğŸ› ï¸ Diagrama de Flujo - CI/CD Voting App
```text
Inicio
â””â”€â”€ ğŸ”¹ Push a rama (dev, test, main)
    â””â”€â”€ ğŸŸ© Determinar entorno
        â”œâ”€â”€ dev â†’ entorno desarrollo
        â”œâ”€â”€ test â†’ entorno testing
        â””â”€â”€ main â†’ entorno producciÃ³n
            â””â”€â”€ ğŸŸ¨ Login a AWS/ECR
                â”œâ”€â”€ aws ecr get-login-password
                â””â”€â”€ docker login con el token generado
                    â””â”€â”€ ğŸŸ§ Generar tag Ãºnico
                        â”œâ”€â”€ obtener hash corto del commit (GIT_COMMIT)
                        â””â”€â”€ Formato: voting-app:<ambiente>-<GIT_COMMIT>
                            â””â”€â”€ ğŸŸ¦ ConstrucciÃ³n de imÃ¡genes
                                â””â”€â”€ docker build -t voting-app:<tag> .
                                    â””â”€â”€ ğŸŸ© Subir imagen a ECR
                                        â”œâ”€â”€ docker tag â†’ apuntar al repo ECR
                                        â””â”€â”€ docker push â†’ subir imagen
                                            â””â”€â”€ ğŸ“ Actualizar archivo docker-compose.generated.yml
                                                â”œâ”€â”€ reemplazar tag de imagen
                                                â””â”€â”€ guardar archivo actualizado
                                                    â””â”€â”€ âœ… Archivo listo para despliegue
                                                        â””â”€â”€ imagen disponible en ECR
                                                            â””â”€â”€ ğŸŸ¨ ConstrucciÃ³n de imagen Lambda
                                                                â””â”€â”€ docker build -t lambda-backup ./lambda-backup
                                                                    â””â”€â”€ ğŸŸ§ Subir imagen a ECR
                                                                        â”œâ”€â”€ docker tag â†’ apuntar al repo ECR
                                                                        â””â”€â”€ docker push â†’ subir imagen
                                                                            â””â”€â”€ ğŸŸ¦ CreaciÃ³n de infra con Terraform
                                                                                â”œâ”€â”€ terraform init y apply: capa network
                                                                                â”œâ”€â”€ tfstate network guardado en bucket
                                                                                â”œâ”€â”€ terraform init y apply: capa ambiente actual
                                                                                â””â”€â”€ tfstate ambiente guardado en bucket
                                                                                    â””â”€â”€ ğŸŸ© Despliegue de Kubernetes
                                                                                        â”œâ”€â”€ reemplazo de variables en manifiestos
                                                                                        â”œâ”€â”€ aws eks update-kubeconfig
                                                                                        â””â”€â”€ kubectl apply -f k8s-specifications
                                                                                            â””â”€â”€ ğŸ” Realizar testing de carga
                                                                                                â”œâ”€â”€ seteo de ambiente
                                                                                                â”œâ”€â”€ corre test en ALB de Vote
                                                                                                â”œâ”€â”€ tabla de restultados
                                                                                                â”œâ”€â”€ corre test en ALB de Results
                                                                                                â””â”€â”€ tabla de restultados
                                                                                                                       â””â”€â”€Î» Invocar Lambda con ALBs
                                                                                                                                                  â””â”€â”€ ğŸ“§ Email notification de lambda result
                                                                                                                                                                                          â””â”€â”€ ğŸ“§ Email notification Resultado del Pipeline
   

```

 ## Terraform Deploy
   - La estructura de infraestructura es la siguiente
   ```text  
        infra/
            env
              |_ dev 
              |    lambda_backup.tf
              |    main.tf
              |    outputs.tf
              |    terraform.tfvars
              |    variables.tf    
              |_ test
              |    lambda_backup.tf
              |    main.tf
              |    outputs.tf
              |    terraform.tfvars
              |    variables.tf    
              |_ main
              |    lambda_backup.tf
              |    main.tf
              |    outputs.tf
              |    terraform.tfvars
              |    variables.tf
            network
                 main.tf
                 output.tf

   
```
Tomamos la decisiÃ³n de esta estructura para la infraestructura por los siguientes motivos:

**SeparaciÃ³n clara por entorno**

Cada entorno (dev, test, main) tiene su propio conjunto de archivos Terraform:
   - Permite aplicar cambios de forma independiente.
   - Reduce el riesgo de errores al evitar que cambios en desarrollo afecten producciÃ³n.
   - Facilita pruebas y validaciones antes de promover cambios.
     
 **ReutilizaciÃ³n**
 
 La carpeta network define infraestructura en comÃºn para todos los ambientes, VPC, IGW, etc. 

 **Escalabilidad**
 
 Es facilmente escalable, se puede agregar nuevos entornos sin modificar los existintes

 **GestiÃ³n de variables por entorno**

 Cada entorno tiene su propio terraform.tfvars, permite definir configuraciones especÃ­ficas (nombres, tamaÃ±os, regiones, etc.) sin duplicar lÃ³gica, mejora la trazabilidad y el control de cambios.
 
 **Cumplimiento y auditorÃ­a**

 Separar entornos ayuda a cumplir con polÃ­ticas de seguridad y auditorÃ­a.

 **PrÃ¡cticas Devops**
 
 Se tomaron en consideraciÃ³n las prÃ¡cticas mÃ¡s comunes de Devops. Cada ambiente tiene su propio cluster EKS (en vez de tener un solo cluster con tres namespaces).

---

ğŸ“Œ *EXTRA* AdemÃ¡s con esta estructura podemos automatizar despliegues por entorno.

---

 ## AnÃ¡lisis estÃ¡tico 
   - Se ejecuta SonarQube en cada push para evaluar calidad de cÃ³digo
   - Se usa el GitHub Action oficial de SonarCloud o configuraciÃ³n personalizada con `sonar-scanner`
   - SonarQube permite mejorar la calidad del cÃ³digo automÃ¡ticamente al analizarlo en busca de errores, vulnerabilidades, cÃ³digo duplicado y malas prÃ¡cticas. Facilita el mantenimiento, reduce el riesgo de fallos en producciÃ³n y promueve buenas prÃ¡cticas de desarrollo mediante mÃ©tricas claras e integraciones con CI/CD. AdemÃ¡s, ayuda a asegurar que el cÃ³digo nuevo  no degrade la calidad existente.

   #### Prerrequisitos SonarQube:
   - Tener un proyecto creado en [SonarCloud](https://sonarcloud.io/) o en tu instancia propia de SonarQube
   - Generar un `SONAR_TOKEN` y agregarlo como *Secret* en GitHub
   - Configurar el archivo `sonar-project.properties` en la raÃ­z del repo, por ejemplo:

     ```properties
     sonar.projectKey=nombre-del-proyecto
     sonar.organization=nombre-organizacion
     sonar.host.url=https://sonarcloud.io
     sonar.login=${SONAR_TOKEN}
     sonar.sources=.
     sonar.language=js
     sonar.sourceEncoding=UTF-8
     ```

   - Verificar que las rutas (`sonar.sources`) coincidan con el cÃ³digo fuente real

Infrome de sonarQube

![Informe_SonarQube.docx](/IMG/Informe_SonarQube.docx)

## Testing
   - Se ejecutan pruebas de carga con JMeter sobre el entorno correspondiente

## Lambda url-checker 

VerificaciÃ³n de disponibilidad de servicios

Esta funciÃ³n Lambda fue desarrollada con el objetivo de monitorear la disponibilidad de los servicios frontend de la Voting App desplegados en AWS (por ejemplo, las aplicaciones vote y result publicadas detrÃ¡s de ALBs).

 ```
/lambda
   |_lambda.zip
 ```

   
Se invoca automÃ¡ticamente desde el pipeline de CI/CD en GitHub Actions, luego del despliegue de infraestructura y servicios, para verificar que las URLs estÃ©n accesibles y respondiendo correctamente.

Permite detectar errores tempranos en el pipeline si algÃºn servicio clave no responde (503, timeout, etc.).

Facilita la automatizaciÃ³n de health checks post-despliegue sin necesidad de herramientas externas.

Aporta visibilidad del estado real de la aplicaciÃ³n al finalizar el CI/CD, integrando:

VerificaciÃ³n HTTP de mÃºltiples endpoints.

Alerta automÃ¡tica por correo en caso de falla.

Seguridad y buenas prÃ¡cticas
La funciÃ³n estÃ¡ empaquetada en ZIP incluyendo la librerÃ­a requests como dependencia externa.

Utiliza verify=False para ignorar certificados autofirmados durante el testeo, evitando falsos negativos en ambientes no productivos.

Responde con un JSON estructurado con los resultados individuales por URL.

La salida de la Lambda es procesada automÃ¡ticamente en el pipeline.

Si alguna URL no responde con 200 OK, el workflow:

Se marca como fallido (exit 1)

EnvÃ­a un correo a un destinatario configurable con detalles del error

## NotificaciÃ³n
   - Se envÃ­a un correo a `$REPO_OWNER_MAIL` con resultados del pipeline y link al despliegue





## ğŸš§ CodeQL y  super-linter como *Quality Gate* en el Proceso de IntegraciÃ³n Continua

Este repositorio utiliza [`codeql-analysis.yml`](.github/workflows/codeql-analysis.yml) para configurar y ejecutar [CodeQL](https://codeql.github.com/), una herramienta de anÃ¡lisis de cÃ³digo estÃ¡tico desarrollada por GitHub, para los siguientes lenguajes 'csharp', 'javascript', 'python'. En este caso, se aplica especÃ­ficamente a la aplicaciÃ³n `voting-app`, con el objetivo de detectar automÃ¡ticamente vulnerabilidades, errores y problemas de calidad en el cÃ³digo de sus distintos servicios.
En este repositorio, CodeQL se utiliza como un **_quality gate_ automÃ¡tico** durante el proceso de integraciÃ³n continua. Esto garantiza que el cÃ³digo que se fusiona en las ramas principales (`dev`, `test` y `prod`) haya pasado un anÃ¡lisis de seguridad y calidad.

### ğŸ” Flujo de trabajo

1. **CreaciÃ³n de un Pull Request hacia `dev`, `test` o `prod`**
   - Cada vez que se propone un cambio hacia alguna de estas ramas, se activa automÃ¡ticamente un anÃ¡lisis CodeQL a travÃ©s de GitHub Actions.

2. **EjecuciÃ³n del anÃ¡lisis de seguridad**
   - CodeQL analiza el cÃ³digo fuente, construye una base de datos interna y ejecuta consultas para detectar:
     - Vulnerabilidades de seguridad
     - Errores de lÃ³gica
     - Problemas comunes de codificaciÃ³n

3. **EvaluaciÃ³n del resultado**
   - Si el anÃ¡lisis detecta alertas crÃ­ticas, el workflow falla y **se bloquea el merge** hasta que se resuelvan los problemas.

4. **Merge aprobado solo si pasa el quality gate**
   - El cÃ³digo solo puede integrarse si pasa exitosamente el anÃ¡lisis CodeQL, asegurando que las ramas clave mantengan un nivel mÃ­nimo de seguridad y calidad.

### âœ… Beneficios

- ğŸ”’ **Seguridad preventiva**: Se bloquean vulnerabilidades antes de llegar a producciÃ³n.
- ğŸ“ **Consistencia**: Se aplica el mismo estÃ¡ndar en todos los entornos (`dev`, `test`, `prod`).
- ğŸ§¹ **ReducciÃ³n de deuda tÃ©cnica**: Se previene la acumulaciÃ³n de errores y malas prÃ¡cticas en el tiempo.
- ğŸš€ **Despliegues mÃ¡s confiables**: Cada rama mantiene un estado seguro y controlado.

---

ğŸ“Œ *EXTRA* Este proceso se complementa con la configuraciÃ³n de **branch protection rules** en GitHub, exigiendo que el anÃ¡lisis CodeQL se complete correctamente antes de permitir merges en las ramas protegidas.

---
Las configuraciones de las **branch protection rules** son las siguientes:

![QG_1.png](QG_1.png)

![QG_2.png](QG_2.png)

### ğŸ§ª Â¿CÃ³mo funciona?

1. **DefiniciÃ³n del flujo de trabajo**  
   El archivo `codeql-analysis.yml` configura la ejecuciÃ³n de CodeQL para los lenguajes utilizados en `voting-app` (por ejemplo, Python y JavaScript).

2. **CreaciÃ³n de la base de datos CodeQL**  
   Se analiza el cÃ³digo fuente de cada servicio y se construye una base de datos con su estructura y semÃ¡ntica.

3. **EjecuciÃ³n de consultas**  
   Se aplican consultas predefinidas y, si es necesario, personalizadas, para identificar vulnerabilidades y errores en los servicios de la aplicaciÃ³n.

4. **PublicaciÃ³n de resultados**  
   Las alertas se muestran automÃ¡ticamente en GitHub, brindando a los desarrolladores informaciÃ³n detallada para remediarlas.

---

ğŸ‘‰ MÃ¡s informaciÃ³n sobre CodeQL: [https://codeql.github.com/docs/](https://codeql.github.com/docs/)

---

## ğŸ“¸ Tablero Kanban

### Primera etapa:

![IMG/Trello 1.png](IMG/Trello%201.png)

### Segunda etapa:

![IMG/Trello 2.png](IMG/Trello%202.png)

### Tercera etapa:



### Decisiones de DiseÃ±o

- Al utilizar el mismo repositorio en el codigo de la aplicaciÃ³n, como en la infraestructura, si en los pipeline (super-linter.yml o codeql-analysis.yml) el resultado es con error, como es un error de cÃ³digo igual se continÃºa con el despliegue de la infraestructura, esto sÃ³lo se aplica para el laboratorio. En el caso del laboratorio codeql-analysis, termina de forma correcta, y super-linter que hace una revision de html,css y yaml no, igual este Ãºltimo sÃ³lo se ejcuta cuando el branch es main.

- En aws se utiliza una sola VPC, para los 3 cluster y 2 subnets por ambiente. Para cada ambiente se tiene un cluster, algunas de las razones fueron menor "Blast Radius", si hay un error humano, una configuraciÃ³n errÃ³nea o un incidente de seguridad en un ambiente, el impacto se limita a ese clÃºster especÃ­fico. Es mucho mÃ¡s difÃ­cil afectar accidentalmente producciÃ³n desde desarrollo. Ciclo de vida y pruebas independientes, puedes probar las actualizaciones de versiÃ³n de Kubernetes en un clÃºster de desarrollo/QA antes de aplicarlas a producciÃ³n. En un solo clÃºster, actualizar la versiÃ³n del Kubernetes afectarÃ­a a todos los ambientes simultÃ¡neamente. Configuraciones de infraestructura especÃ­ficas, cada clÃºster puede tener configuraciones de red, almacenamiento, balanceadores de carga o tipos de instancias subyacentes optimizadas para las necesidades especÃ­ficas de ese ambiente (ej: menor costo en dev, alta disponibilidad y performance en prod).

- 
