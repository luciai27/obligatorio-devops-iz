# ğŸ—³ï¸ Obligatorio Voting App

## ğŸ› ï¸ Herramientas
- **Repositorio:** GitHub  
- **CI/CD:** GitHub Actions  
- **AnÃ¡lisis de cÃ³digo estÃ¡tico:** SonarQube  
- **Cloud:** AWS  
- **Infraestructura como CÃ³digo (IaC):** Terraform  
- **Testing:** JMeter
- **Serverless:** Lambda 

---
## ğŸ” Prerequisitos
Estas variables deben estar configuradas como *Secrets* en GitHub:

- `AWS_ACCESS_KEY_ID`
- `AWS_REGION`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_SESSION_TOKEN`
- `BUCKET_NAME` (nombre Ãºnico del bucket S3 en donde se guardarÃ¡n los tfstates.)
- `EMAIL_USER`
- `EMAIL_PASS`
- `REPO_OWNER_MAIL`
- `SONAR_TOKEN`

Los primeros cuatro secretos corresponden a configuraciones de AWS, por lo que siempre son necesarios para poder acceder correctamente a los servicios.

El Bucket Name es necesario ya que, al ser dos personas las que estamos trabajando en el proyecto y dado que los buckets de S3 deben tener nombres Ãºnicos, no es posible utilizar un mismo nombre en cuentas diferentes. Esto fue algo que se tuvo que parametrizar (y, por lo tanto, el mismo debe ser creado manualmente **antes** de correr el pipeline).

Los secretos de Email User, Email Pass y Repo Owner Mail son necesarios para el envÃ­o de correo cuando se crea un Pull Request y cuando finaliza lambda.

El Sonar Token es necesario para la realizaciÃ³n del anÃ¡lisis de cÃ³digo de SonarQube.


---

## ğŸŒ¿ Estrategia Git Flow

La estrategia elegida fue **Git Flow**. Si bien entendemos que la estrategia **Trunk Based** tiene caracterÃ­sticas Ãºtiles (promueve integraciÃ³n continua, especialmente Ãºtil en proyectos pequeÃ±os), decidimos utilizar **Git Flow** ya que nos permite observar mÃ¡s atentamente los cambios realizados a la rama principal.

Dado que todavÃ­a estamos aprendiendo cÃ³mo utilizar las tecnologÃ­as enseÃ±adas en clase, consideramos que un monitoreo mÃ¡s a fondo de lo que se incorpora a la rama principal es la estrategia que mÃ¡s se alinea con nuestra forma de trabajo. Al utilizar esta estrategia, sabemos que lo que se integra a `main` estÃ¡ funcionando correctamente.

### âœ… Entornos bien definidos y separados
El proyecto tiene ramas bien diferenciadas que se alinean con Git Flow:
- `dev`: para desarrollo
- `test`: para validaciÃ³n antes de producciÃ³n
- `main`: versiÃ³n estable y en producciÃ³n

### ğŸ“¦ Control sobre versiones y despliegues
Git Flow permite:
- Controlar cuÃ¡ndo se libera una nueva versiÃ³n
- Aplicar hotfixes sin afectar `dev`
- Mantener la estabilidad en `main` mientras se desarrollan nuevas funcionalidades

### ğŸ” IntegraciÃ³n con flujos CI/CD por ramas
- La app genera imÃ¡genes por rama (`dev`, `test`, `main`)
- Cada rama despliega en su entorno especÃ­fico
- Git Flow encaja naturalmente con pipelines CI/CD basados en tags por rama

### ğŸ›¡ï¸ AislaciÃ³n de features y bugs
- Ramas especÃ­ficas para nuevas features sin romper `dev`
- Hotfixes crÃ­ticos directamente sobre `main`
- Mayor seguridad antes de llegar a producciÃ³n

---



## ğŸ“ Estrategia de Repositorio para Infraestructura

Decidimos usar **el mismo repositorio** para la carpeta de infraestructura. 

Esto nos resulta mÃ¡s prÃ¡ctico para un proyecto pequeÃ±o como este, ya que podemos realizar cambios tanto en la aplicaciÃ³n como en la infraestructura desde un mismo lugar. Si el proyecto fuera mÃ¡s grande, sÃ­ considerarÃ­amos separar el cÃ³digo de infraestructura en un repositorio exclusivo para facilitar su reutilizaciÃ³n.

---

## ğŸ§± Arquitectura

![Architecture diagram](architecture.excalidraw.png)

Componentes:
- ğŸ Front-end en [Python](/vote): permite votar entre dos opciones
- ğŸ§  [Redis](https://hub.docker.com/_/redis/): almacena los votos temporales
- âš™ï¸ [Worker en .NET](/worker): consume votos desde Redis y los guarda en...
- ğŸ›¢ï¸ [Postgres](https://hub.docker.com/_/postgres/): base de datos persistente
- ğŸ“Š Web app [Node.js](/result): muestra resultados de la votaciÃ³n en tiempo real

---

## ğŸš€ Flow de CI/CD

1. **Push a una rama (`dev`, `test`, `main`)**
   - Se configuran credenciales AWS
   - Se crean repositorios ECR para imÃ¡genes
   - Se genera una nueva imagen Docker con tag Ãºnico
   - Se sube la imagen a ECR con tag de entorno
   - Se actualiza el archivo `docker-compose.generated.yml` con el tag generado
   - El archivo `docker-compose.generated.yml` se sube a bucket S3
   - Se crea la infraestructura comÃºn a todos los ambientes (network)
   - Se crea la infrastructura correspondiente al ambiente del push
   - Se remplazan variables y se despliegan manifiestos K8s
   - Se prepara ambiente para testing
   - Se buscan los URL de los ALBs y setean como variables
   - Se realiza testing de carga en ALBs creados por K8s (Vote y Result)
   - Se invoca funciÃ³n Lambda para verificaciÃ³n de estado de URLs
   - Se procesan resultados y se envÃ­a notificaciÃ³n por correo electrÃ³nico

ğŸ› ï¸ Diagrama de Flujo - CI/CD Voting App
```text
Inicio
â””â”€â”€ ğŸ”¹ Push a rama (dev, test, main)
    â””â”€â”€ ğŸŸ© Determinar entorno
        â”œâ”€â”€ dev â†’ entorno desarrollo
        â”œâ”€â”€ test â†’ entorno testing
        â””â”€â”€ main â†’ entorno producciÃ³n
            â””â”€â”€ ğŸŸ¨ Login a AWS/ECR
                â”œâ”€â”€ aws ecr get-login-password
                â””â”€â”€ docker login con el token generado
                    â””â”€â”€ ğŸŸ§ Generar tag Ãºnico
                        â”œâ”€â”€ obtener hash corto del commit (GIT_COMMIT)
                        â””â”€â”€ Formato: voting-app:<ambiente>-<GIT_COMMIT>
                            â””â”€â”€ ğŸŸ¦ ConstrucciÃ³n de imÃ¡genes
                                â””â”€â”€ docker build -t voting-app:<tag> .
                                    â””â”€â”€ ğŸŸ© Subir imagen a ECR
                                        â”œâ”€â”€ docker tag â†’ apuntar al repo ECR
                                        â””â”€â”€ docker push â†’ subir imagen
                                            â””â”€â”€ ğŸ“ Actualizar archivo docker-compose.generated.yml
                                                â”œâ”€â”€ reemplazar tag de imagen
                                                â””â”€â”€ guardar archivo actualizado
                                                    â””â”€â”€ âœ… Archivo listo para despliegue
                                                        â””â”€â”€ imagen disponible en ECR
                                                            â””â”€â”€ ğŸŸ¦ CreaciÃ³n de infra con Terraform
                                                                â”œâ”€â”€ terraform init y apply: capa network
                                                                â”œâ”€â”€ tfstate network guardado en bucket
                                                                â”œâ”€â”€ terraform init y apply: capa ambiente actual
                                                                â””â”€â”€ tfstate ambiente guardado en bucket
                                                                    â””â”€â”€ ğŸŸ© Despliegue de Kubernetes
                                                                        â”œâ”€â”€ reemplazo de variables en manifiestos
                                                                        â”œâ”€â”€ aws eks update-kubeconfig
                                                                        â””â”€â”€ kubectl apply -f k8s-specifications
                                                                            â””â”€â”€ ğŸ” Seteo de ambiente y config para Testing
                                                                                â””â”€â”€ ObtenciÃ³n de URL de ALBs
                                                                                    â”œâ”€â”€ busca por puerto 8080
                                                                                    â”œâ”€â”€ setea dependiendo del ambiente
                                                                                    â”œâ”€â”€ busca por puerto 8081
                                                                                    â””â”€â”€ setea dependiendo del ambiente
                                                                                         â””â”€â”€ğŸ” Corre testing               
                                                                                            â”œâ”€â”€ carga para ALB vote
                                                                                            â”œâ”€â”€ carga para ALB result
                                                                                            â””â”€â”€ QG: pasa si success = %100
                                                                                                â””â”€â”€Î» Invocar Lambda con ALBs
                                                                                                    â”œâ”€â”€ check URL vote OK
                                                                                                    â””â”€â”€ check URL result OK
                                                                                                        â””â”€â”€ ğŸ“§ Email notification de lambda result
                                                                                                            â””â”€â”€ ğŸ“§ Email notification Resultado del Pipeline


```

 ## Terraform Deploy
   - La estructura de infraestructura es la siguiente
   ```text  
        infra/
            env
              |_ dev 
              |    lambda_backup.tf
              |    main.tf
              |    outputs.tf
              |    terraform.tfvars
              |    variables.tf    
              |_ test
              |    lambda_backup.tf
              |    main.tf
              |    outputs.tf
              |    terraform.tfvars
              |    variables.tf    
              |_ main
              |    lambda_backup.tf
              |    main.tf
              |    outputs.tf
              |    terraform.tfvars
              |    variables.tf
            network
                 main.tf
                 output.tf

   
```
Tomamos la decisiÃ³n de esta estructura para la infraestructura por los siguientes motivos:

**SeparaciÃ³n clara por entorno**

Cada entorno (dev, test, main) tiene su propio conjunto de archivos Terraform:
   - Permite aplicar cambios de forma independiente.
   - Reduce el riesgo de errores al evitar que cambios en desarrollo afecten producciÃ³n.
   - Facilita pruebas y validaciones antes de promover cambios.
     
 **ReutilizaciÃ³n**
 
 La carpeta network define infraestructura en comÃºn para todos los ambientes, VPC, IGW, etc. 

 **Escalabilidad**
 
 Es facilmente escalable, se puede agregar nuevos entornos sin modificar los existintes

 **GestiÃ³n de variables por entorno**

 Cada entorno tiene su propio terraform.tfvars, permite definir configuraciones especÃ­ficas (nombres, tamaÃ±os, regiones, etc.) sin duplicar lÃ³gica, mejora la trazabilidad y el control de cambios.
 
 **Cumplimiento y auditorÃ­a**

 Separar entornos ayuda a cumplir con polÃ­ticas de seguridad y auditorÃ­a.

 **PrÃ¡cticas Devops**
 
 Se tomaron en consideraciÃ³n las prÃ¡cticas mÃ¡s comunes de Devops. Cada ambiente tiene su propio cluster EKS (en vez de tener un solo cluster con tres namespaces).

---

ğŸ“Œ *EXTRA* AdemÃ¡s con esta estructura podemos automatizar despliegues por entorno.

---

 ## AnÃ¡lisis estÃ¡tico 
   - Se ejecuta SonarQube en cada push para evaluar calidad de cÃ³digo
   - Se usa el GitHub Action oficial de SonarCloud o configuraciÃ³n personalizada con `sonar-scanner`
   - SonarQube permite mejorar la calidad del cÃ³digo automÃ¡ticamente al analizarlo en busca de errores, vulnerabilidades, cÃ³digo duplicado y malas prÃ¡cticas. Facilita el mantenimiento, reduce el riesgo de fallos en producciÃ³n y promueve buenas prÃ¡cticas de desarrollo mediante mÃ©tricas claras e integraciones con CI/CD. AdemÃ¡s, ayuda a asegurar que el cÃ³digo nuevo  no degrade la calidad existente.

   #### Prerrequisitos SonarQube:
   - Tener un proyecto creado en [SonarCloud](https://sonarcloud.io/) o en tu instancia propia de SonarQube
   - Generar un `SONAR_TOKEN` y agregarlo como *Secret* en GitHub
   - Configurar el archivo `sonar-project.properties` en la raÃ­z del repo, por ejemplo:

     ```properties
     sonar.projectKey=nombre-del-proyecto
     sonar.organization=nombre-organizacion
     sonar.host.url=https://sonarcloud.io
     sonar.login=${SONAR_TOKEN}
     sonar.sources=.
     sonar.language=js
     sonar.sourceEncoding=UTF-8
     ```

   - Verificar que las rutas (`sonar.sources`) coincidan con el cÃ³digo fuente real

Infrome de sonarQube

![Informe_SonarQube.docx](/IMG/Informe_SonarQube.docx)

## Testing
Para la realizaciÃ³n del testing del obligatorio se optÃ³ por pruebas de carga utilizando JMeter. Se utilizÃ³ BlazeMeter con Taurus, lo que permitiÃ³ incluir un failure criteria para que el testing no continuara si fallaba una sola prueba.
La prueba de carga que se realizÃ³ se encuentra en el archivo test.jmx y consiste en lo siguiente:

- `<intProp name="ThreadGroup.num_threads">10</intProp>`: el nÃºmero de threads (usuarios) es 10.
- `<intProp name="ThreadGroup.ramp_time">5</intProp>`: JMeter demora 5 segundos para que se conecten los 10 usuarios.
- `<longProp name="ThreadGroup.duration">15</longProp>`: la duraciÃ³n total del test es de 15 segundos.


## Lambda url-checker 

VerificaciÃ³n de disponibilidad de servicios

Esta funciÃ³n Lambda fue desarrollada con el objetivo de monitorear la disponibilidad de los servicios frontend de la Voting App desplegados en AWS (por ejemplo, las aplicaciones vote y result publicadas detrÃ¡s de ALBs).

 ```
/lambda
   |_lambda.zip
 ```

   
Se invoca automÃ¡ticamente desde el pipeline de CI/CD en GitHub Actions, luego del despliegue de infraestructura y servicios, para verificar que las URLs estÃ©n accesibles y respondiendo correctamente.

Permite detectar errores tempranos en el pipeline si algÃºn servicio clave no responde (503, timeout, etc.).

Facilita la automatizaciÃ³n de health checks post-despliegue sin necesidad de herramientas externas.

Aporta visibilidad del estado real de la aplicaciÃ³n al finalizar el CI/CD, integrando:

VerificaciÃ³n HTTP de mÃºltiples endpoints.

Alerta automÃ¡tica por correo en caso de falla.

Seguridad y buenas prÃ¡cticas
La funciÃ³n estÃ¡ empaquetada en ZIP incluyendo la librerÃ­a requests como dependencia externa.

Utiliza verify=False para ignorar certificados autofirmados durante el testeo, evitando falsos negativos en ambientes no productivos.

Responde con un JSON estructurado con los resultados individuales por URL.

La salida de la Lambda es procesada automÃ¡ticamente en el pipeline.

Si alguna URL no responde con 200 OK, el workflow:

Se marca como fallido (exit 1)

EnvÃ­a un correo a un destinatario configurable con detalles del error

## NotificaciÃ³n
   - Se envÃ­a un correo a `$REPO_OWNER_MAIL` con resultados del pipeline y link al despliegue


## Cloudwatch


## ğŸš§ CodeQL y  super-linter como *Quality Gate* en el Proceso de IntegraciÃ³n Continua

Este repositorio utiliza [`codeql-analysis.yml`](.github/workflows/codeql-analysis.yml) para configurar y ejecutar [CodeQL](https://codeql.github.com/), una herramienta de anÃ¡lisis de cÃ³digo estÃ¡tico desarrollada por GitHub, para los siguientes lenguajes 'csharp', 'javascript', 'python'. En este caso, se aplica especÃ­ficamente a la aplicaciÃ³n `voting-app`, con el objetivo de detectar automÃ¡ticamente vulnerabilidades, errores y problemas de calidad en el cÃ³digo de sus distintos servicios.
En este repositorio, CodeQL se utiliza como un **_quality gate_ automÃ¡tico** durante el proceso de integraciÃ³n continua. Esto garantiza que el cÃ³digo que se fusiona en las ramas principales (`dev`, `test` y `prod`) haya pasado un anÃ¡lisis de seguridad y calidad.

### ğŸ” Flujo de trabajo

1. **CreaciÃ³n de un Pull Request hacia `dev`, `test` o `prod`**
   - Cada vez que se propone un cambio hacia alguna de estas ramas, se activa automÃ¡ticamente un anÃ¡lisis CodeQL a travÃ©s de GitHub Actions.

2. **EjecuciÃ³n del anÃ¡lisis de seguridad**
   - CodeQL analiza el cÃ³digo fuente, construye una base de datos interna y ejecuta consultas para detectar:
     - Vulnerabilidades de seguridad
     - Errores de lÃ³gica
     - Problemas comunes de codificaciÃ³n

3. **EvaluaciÃ³n del resultado**
   - Si el anÃ¡lisis detecta alertas crÃ­ticas, el workflow falla y **se bloquea el merge** hasta que se resuelvan los problemas.

4. **Merge aprobado solo si pasa el quality gate**
   - El cÃ³digo solo puede integrarse si pasa exitosamente el anÃ¡lisis CodeQL, asegurando que las ramas clave mantengan un nivel mÃ­nimo de seguridad y calidad.

### âœ… Beneficios

- ğŸ”’ **Seguridad preventiva**: Se bloquean vulnerabilidades antes de llegar a producciÃ³n.
- ğŸ“ **Consistencia**: Se aplica el mismo estÃ¡ndar en todos los entornos (`dev`, `test`, `prod`).
- ğŸ§¹ **ReducciÃ³n de deuda tÃ©cnica**: Se previene la acumulaciÃ³n de errores y malas prÃ¡cticas en el tiempo.
- ğŸš€ **Despliegues mÃ¡s confiables**: Cada rama mantiene un estado seguro y controlado.

---

ğŸ“Œ *EXTRA* Este proceso se complementa con la configuraciÃ³n de **branch protection rules** en GitHub, exigiendo que el anÃ¡lisis CodeQL se complete correctamente antes de permitir merges en las ramas protegidas.

---
Las configuraciones de las **branch protection rules** son las siguientes:

![QG_1.png](/IMG/QG_1.png)

![QG_2.png](/IMG/QG_2.png)

### ğŸ§ª Â¿CÃ³mo funciona?

1. **DefiniciÃ³n del flujo de trabajo**  
   El archivo `codeql-analysis.yml` configura la ejecuciÃ³n de CodeQL para los lenguajes utilizados en `voting-app` (por ejemplo, Python y JavaScript).

2. **CreaciÃ³n de la base de datos CodeQL**  
   Se analiza el cÃ³digo fuente de cada servicio y se construye una base de datos con su estructura y semÃ¡ntica.

3. **EjecuciÃ³n de consultas**  
   Se aplican consultas predefinidas y, si es necesario, personalizadas, para identificar vulnerabilidades y errores en los servicios de la aplicaciÃ³n.

4. **PublicaciÃ³n de resultados**  
   Las alertas se muestran automÃ¡ticamente en GitHub, brindando a los desarrolladores informaciÃ³n detallada para remediarlas.

---

ğŸ‘‰ MÃ¡s informaciÃ³n sobre CodeQL: [https://codeql.github.com/docs/](https://codeql.github.com/docs/)

---

## ğŸ“¸ Tablero Kanban

### Primera etapa:

![IMG/Trello 1.png](IMG/Trello%201.png)

### Segunda etapa:

![IMG/Trello 2.png](IMG/Trello%202.png)

### Tercera etapa:

![IMG/Trello 3.png](IMG/Trello%203.png)


### Decisiones de DiseÃ±o

- Como se mencionÃ³ anteriormente en la documentaciÃ³n, se incluyÃ³ tanto la infraestructura, como el cÃ³digo de la aplicaciÃ³n en el mismo repositorio ya que, en nuestro parecer, es un proyecto pequeÃ±o que se beneficiÃ³ de solamente tener un lugar de trabajo. Dado que fue nuestro primer intento de despliegue automatizado de infraestructura utilizando IaC, nos resultÃ³ Ãºtil tener ambas Ã¡reas juntas y en contante testeo.

- Se utilizÃ³ un solo workflow para todos los ambientes. Se parametrizÃ³ el ambiente del cual provino el push, lo que brinda mayor flexibilidad si se desean incluir mÃ¡s branches en el repositorio, ya que no serÃ¡ necesario crear workflows dedicados para las nuevas ramas, simplemente se deben contemplan sus nombres en el condicional inicial del workflow Ãºnico.

- En AWS se utilizÃ³ una sola VPC con 6 subnets pÃºblicas (2 por cada ambiente: dev, test y prod) y 3 clusters EKS (tambiÃ©n uno por ambiente). Algunas de las razones que nos llevaron a tomar estas deciciones fueron: 
    - Menor "Blast Radius": si hay un error humano, una configuraciÃ³n errÃ³nea o un incidente de seguridad en un ambiente, el impacto se limita a ese clÃºster especÃ­fico. Es mucho mÃ¡s difÃ­cil afectar accidentalmente el ambiente de producciÃ³n desde desarrollo. 
    - Ciclo de vida y pruebas independientes: se pueden probar las actualizaciones de versiÃ³n de Kubernetes en un clÃºster de desarrollo/QA antes de aplicarlas a producciÃ³n. En un solo clÃºster, actualizar la versiÃ³n del Kubernetes afectarÃ­a a todos los ambientes simultÃ¡neamente. 
    - Configuraciones de infraestructura especÃ­ficas: cada clÃºster puede tener configuraciones de red, almacenamiento, balanceadores de carga o tipos de instancias subyacentes optimizadas para las necesidades especÃ­ficas de ese ambiente (ej: menor costo en dev, alta disponibilidad y performance en prod). Si bien en el obligatorio se utilizaron las mismas propiedades en todos los ambientes, se tuvo este punto en cuenta.
    - Especificaciones de EKS: Se utilizaron 2 subnets pÃºblicas ya que fue el menor nÃºmero de subnets permitidas por EKS para la creaciÃ³n de clusters.

- Para la IaC no se utilizaron mÃ³dulos pero se utilizÃ³ el mismo contenido del main.tf, solamente con variables diferenciadas por ambiente, lo que facilitÃ³ la realizaciÃ³n de cambios y nos otorgÃ³ flexibilidad.

- No se utilizÃ³ la estrategia de feature branch para el desarrollo de la infraestructura dado que se creÃ³ en el mismo repositorio que la aplicaciÃ³n y no nos restulÃ³ prÃ¡ctico utilizar esta estrategia durante el transcurso del proyeto.

- El testing de carga se aplicÃ³ como quality gate, es decir, si el mismo falla, se cancela el resto del pipeline. El failure criteria se estableciÃ³ en menos de "100% success", o sea, mientras nada falle, seguirÃ¡ el pipeline.

- Si los pipelines de "super-linter.yml" o "codeql-analysis.yml" no llegan a completarse, esto no contituye un error, ya que se continÃºa con el despliegue de la infraestructura. En el caso de "codeql-analysis", Ã©ste termina de forma correcta, mientras que para "super-linter", es posible que no se complete dado que es una revision de HTML, CSS y otros archivos de cÃ³digo, que no nos corresponde arreglar en el presente obligatorio.

### Lecciones aprendidas

- Al principio luchamos mucho con la lÃ³gica y la creaciÃ³n de la Infraestructura como CÃ³digo, ya que estÃ¡bamos tratando de crear subnets privadas y pÃºblicas conectadas a travÃ©s de NAT gateways para mantener la seguridad de los clusters. Nos dimos cuenta que a veces menos es mÃ¡s, por lo menos en el caso del obligatorio. Nos gustarÃ­a poder modificarlo luego con una infraestructura similar a la mencionada.

- A pesar de que menos es mÃ¡s, en el caso de la bÃºsqueda de los ALBs en el pipeline de CI/CD, nos dimos cuenta tambiÃ©n que nada es imposible. Cuando comenzamos a desarrollar el pipeline, los ALBs creados automÃ¡ticamente por los manifiestos kubernetes se ingresaban manualmente como secretos del repositorio, lo que significaba que el mismo se iba a romper cuando llegara a la parte del testing por primera vez. Una vez roto, se ingresarÃ­an los ALBs creados en el step anterior (despliegue de K8s) y luego se correrÃ­a nuevamente el pipeline. Insatisfechos con esto, buscamos una soluciÃ³n utilizando el AWS API (query "LoadBalancerDescriptions[*].[LoadBalancerName,DNSName]"). Esto no fue suficiente, ya que, si bien traÃ­a los ALBs creados, no los podÃ­a filtrar por fecha de creaciÃ³n por ser ALBs clÃ¡sicos (la API no soporta esta condiciÃ³n para este tipo de Load Balancer), cosa que necesitÃ¡bamos para poder diferenciar los ALBs de los diferentes ambientes. Luego de mucho trabajo, llegamos a la lÃ­nea "readarray -t dns_array_8080 < <( aws elb describe-load-balancers --output json | jq -r '.LoadBalancerDescriptions[] | select(.ListenerDescriptions[].Listener.LoadBalancerPort == 8080) | "\(.CreatedTime) \(.DNSName)"' | sort | awk '{print $2}')", que fue indispensable para la automatizaciÃ³n del resto del pipeline. BÃ¡sicamente, creamos un array con todos los ALBs (en este caso filtrados por puerto 8080, pero se realizÃ³ lo mismo para el puerto 8081), los cuales estÃ¡n ordenados por fecha de creaciÃ³n. Luego se crearon condicionales para determinar quÃ© ALBs se tomarÃ­an basado en la posiciÃ³n en la que se encontraba (dado que nuestro repositorio sigue el orden de PR dev -> test -> prod, sabÃ­amos que el mÃ¡s antiguo serÃ­a el de dev y el mÃ¡s nuevo, de prod). La posiciÃ³n 0 serÃ¡ de los ALBs de dev, la 1 de test y la 2 de prod.